{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "<h2>Project 3: Na&iuml;ve Bayes and the Perceptron</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "<blockquote>\n",
    "    <center>\n",
    "    <img src=\"nb.png\" width=\"200px\" />\n",
    "    </center>\n",
    "      <p><cite><center>\"All models are wrong, but some are useful.\"<br>\n",
    "       -- George E.P. Box\n",
    "      </center></cite></p>\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "<h3>Introduction</h3>\n",
    "<!--AÃ°albrandr-->\n",
    "\n",
    "<p>A&eth;albrandr is visiting America from Norway and has been having the hardest time distinguishing boys and girls because of the weird American names like Jack and Jane.  This has been causing lots of problems for A&eth;albrandr when he goes on dates. When he heard that Cornell has a Machine Learning class, he asked that we help him identify the gender of a person based on their name to the best of our ability.  In this project, you will implement Na&iuml;ve Bayes to predict if a name is male or female. </p>\n",
    "\n",
    "<strong>How to submit:</strong> You can submit your code using the red <strong>Submit</strong> button above. This button will send any code below surrounded by <strong>#&lt;GRADED&gt;</strong><strong>#&lt;/GRADED&gt;</strong> tags below to the autograder, which will then run several tests over your code. By clicking on the <strong>Details</strong> dropdown next to the Submit button, you will be able to view your submission report once the autograder has completed running. This submission report contains a summary of the tests you have failed or passed, as well as a log of any errors generated by your code when we ran it.\n",
    "\n",
    "Note that this may take a while depending on how long your code takes to run! Once your code is submitted you may navigate away from the page as you desire -- the most recent submission report will always be available from the Details menu.\n",
    "\n",
    "<p><strong>Evaluation:</strong> Your code will be autograded for technical\n",
    "correctness and--on some assignments--speed. Please <em>do not</em> change the names of any provided functions or classes within the code, or you will wreak havoc on the autograder. Furthermore, <em>any code not surrounded by <strong>#&lt;GRADED&gt;</strong><strong>#&lt;/GRADED&gt;</strong> tags will not be run by the autograder</em>. However, the correctness of your implementation -- not the autograder's output -- will be the final judge of your score.  If necessary, we will review and grade assignments individually to ensure that you receive due credit for your work.\n",
    "\n",
    "<p><strong>Academic Integrity:</strong> We will be checking your code against other submissions in the class for logical redundancy. If you copy someone else's code and submit it with minor changes, we will know. These cheat detectors are quite hard to fool, so please don't try. We trust you all to submit your own work only; <em>please</em> don't let us down. If you do, we will pursue the strongest consequences available to us.\n",
    "\n",
    "<p><strong>Getting Help:</strong> You are not alone!  If you find yourself stuck  on something, contact the course staff for help.  Office hours, section, and the <a href=\"https://piazza.com/class/iyag4nk2rsxsv\">Piazza</a> are there for your support; please use them.  If you can't make our office hours, let us know and we will schedule more.  We want these projects to be rewarding and instructional, not frustrating and demoralizing.  But, we don't know when or how to help unless you ask.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "<h3> Of boys and girls </h3>\n",
    "\n",
    "<p> Take a look at the files <code>girls.train</code> and <code>boys.train</code>. For example with the unix command <pre>cat girls.train</pre> \n",
    "<pre>\n",
    "...\n",
    "Addisyn\n",
    "Danika\n",
    "Emilee\n",
    "Aurora\n",
    "Julianna\n",
    "Sophia\n",
    "Kaylyn\n",
    "Litzy\n",
    "Hadassah\n",
    "</pre>\n",
    "Believe it or not, these are all more or less common girl names. The problem with the current file is that the names are in plain text, which makes it hard for a machine learning algorithm to do anything useful with them. We therefore need to transform them into some vector format, where each name becomes a vector that represents a point in some high dimensional input space. </p>\n",
    "\n",
    "<p>That is exactly what the following Python function <code>name2features</code> does: </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#<GRADED>\n",
    "import numpy as np\n",
    "#</GRADED>\n",
    "import sys\n",
    "\n",
    "# add p03 folder\n",
    "sys.path.insert(0, './p03/')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def hashfeatures(baby, B, FIX):\n",
    "    v = np.zeros(B)\n",
    "    for m in range(FIX):\n",
    "        featurestring = \"prefix\" + baby[:m]\n",
    "        v[hash(featurestring) % B] = 1\n",
    "        featurestring = \"suffix\" + baby[-1*m:]\n",
    "        v[hash(featurestring) % B] = 1\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def name2features(filename, B=128, FIX=3, LoadFile=True):\n",
    "    \"\"\"\n",
    "    Output:\n",
    "    X : n feature vectors of dimension B, (nxB)\n",
    "    \"\"\"\n",
    "    # read in baby names\n",
    "    if LoadFile:\n",
    "        with open(filename, 'r') as f:\n",
    "            babynames = [x.rstrip() for x in f.readlines() if len(x) > 0]\n",
    "    else:\n",
    "        babynames = filename.split('\\n')\n",
    "    n = len(babynames)\n",
    "    X = np.zeros((n, B))\n",
    "    for i in range(n):\n",
    "        X[i,:] = hashfeatures(babynames[i], B, FIX)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "It reads every name in the given file and converts it into a 128-dimensional feature vector. </p> \n",
    "\n",
    "<p>Can you figure out what the features are? (Understanding how these features are constructed will help you later on in the competition.)<br></p>\n",
    "\n",
    "<p>We have provided you with a python function <code>genTrainFeatures</code>, which calls this script, transforms the names into features and loads them into memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def genTrainFeatures(dimension=128, fix=3):\n",
    "    \"\"\"\n",
    "    function [x,y]=genTrainFeatures\n",
    "    \n",
    "    This function calls the python script \"name2features.py\" \n",
    "    to convert names into feature vectors and loads in the training data. \n",
    "    \n",
    "    \n",
    "    Output: \n",
    "    x: n feature vectors of dimensionality d [d,n]\n",
    "    y: n labels (-1 = girl, +1 = boy)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load in the data\n",
    "    Xgirls = name2features(\"girls.train\", B=dimension, FIX=fix)\n",
    "    Xboys = name2features(\"boys.train\", B=dimension, FIX=fix)\n",
    "    X = np.concatenate([Xgirls, Xboys])\n",
    "    \n",
    "    # Generate Labels\n",
    "    Y = np.concatenate([-np.ones(len(Xgirls)), np.ones(len(Xboys))])\n",
    "    \n",
    "    # shuffle data into random order\n",
    "    ii = np.random.permutation([i for i in range(len(Y))])\n",
    "    \n",
    "    return X[ii, :], Y[ii]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "You can call the following command to load in the features and the labels of all boys and girls names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "locked": false,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "X,Y = genTrainFeatures(128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "<h3> The Na&iuml;ve Bayes Classifier </h3>\n",
    "\n",
    "<p> The Na&iuml;ve Bayes classifier is a linear classifier based on Bayes Rule. The following questions will ask you to finish these functions in a pre-defined order. <br>\n",
    "<strong>As a general rule, you should avoid tight loops at all cost.</strong></p>\n",
    "<p>(a) Estimate the class probability P(Y) in \n",
    "<b><code>naivebayesPY</code></b>\n",
    ". This should return the probability that a sample in the training set is positive or negative, independent of its features.\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#<GRADED>\n",
    "def naivebayesPY(x,y):\n",
    "    \"\"\"\n",
    "    function [pos,neg] = naivebayesPY(x,y);\n",
    "\n",
    "    Computation of P(Y)\n",
    "    Input:\n",
    "        x : n input vectors of d dimensions (nxd)\n",
    "        y : n labels (-1 or +1) (nx1)\n",
    "\n",
    "    Output:\n",
    "    pos: probability p(y=1)\n",
    "    neg: probability p(y=-1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # add one positive and negative example to avoid division by zero (\"plus-one smoothing\")\n",
    "    y = np.concatenate([y, [-1,1]])\n",
    "    n = len(y)\n",
    "    \n",
    "    ## fill in code here\n",
    "    pos = list(y).count(1) * 1.0 / n\n",
    "    return pos, 1-pos\n",
    "\n",
    "#</GRADED>\n",
    "\n",
    "pos,neg = naivebayesPY(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "<p>(b) Estimate the conditional probabilities P(X|Y) in \n",
    "<b><code>naivebayesPXY</code></b>\n",
    ".  Use a <b>multinomial</b> distribution as model. This will return the probability vectors  for all features given a class label.\n",
    "</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#<GRADED>\n",
    "def naivebayesPXY(x,y):\n",
    "    \"\"\"\n",
    "    function [posprob,negprob] = naivebayesPXY(x,y);\n",
    "    \n",
    "    Computation of P(X|Y)\n",
    "    Input:\n",
    "        x : n input vectors of d dimensions (nxd)\n",
    "        y : n labels (-1 or +1) (nx1)\n",
    "    \n",
    "    Output:\n",
    "    posprob: probability vector of p(x|y=1) (1xd)\n",
    "    negprob: probability vector of p(x|y=-1) (1xd)\n",
    "    \"\"\"\n",
    "    \n",
    "    # add one positive and negative example to avoid division by zero (\"plus-one smoothing\")\n",
    "    n, d = x.shape\n",
    "    x = np.concatenate([x, np.ones((2,d))])\n",
    "    y = np.concatenate([y, [-1,1]])\n",
    "    n, d = x.shape\n",
    "    \n",
    "    ## fill in code here\n",
    "    #n_pos = list(y).count(1) * 1.0\n",
    "    #n_neg = list(y).count(-1) * 1.0\n",
    "    \n",
    "    \n",
    "    pos = np.array([x[i] for i in range(n) if y[i] == 1])\n",
    "    neg = np.array([x[i] for i in range(n) if y[i] == -1])\n",
    "    pos_total = np.sum(pos)*1.0\n",
    "    neg_total = np.sum(neg)*1.0\n",
    "    \n",
    "    posprob = np.divide(np.sum(pos, axis=0), pos_total)\n",
    "    negprob = np.divide(np.sum(neg, axis=0), neg_total)\n",
    "    \n",
    "    return posprob, negprob\n",
    "    \n",
    "        \n",
    "    \n",
    "#</GRADED>\n",
    "\n",
    "posprob,negprob = naivebayesPXY(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#<GRADED>\n",
    "def naivebayes(x,y,xtest):\n",
    "    \"\"\"\n",
    "    function logratio = naivebayes(x,y);\n",
    "    \n",
    "    Computation of log P(Y|X=x1) using Bayes Rule\n",
    "    Input:\n",
    "    x : n input vectors of d dimensions (nxd)\n",
    "    y : n labels (-1 or +1)\n",
    "    xtest: input vector of d dimensions (1xd)\n",
    "    \n",
    "    Output:\n",
    "    logratio: log (P(Y = 1|X=xtest)/P(Y=-1|X=xtest))\n",
    "    \"\"\"\n",
    "    \n",
    "    ## fill in code here\n",
    "    posprob, negprob = naivebayesPXY(x,y)\n",
    "    pos, neg = naivebayesPY(x,y)\n",
    "    xtest_idx = list(np.where(xtest == 1)[0])\n",
    "    \n",
    "    numerator = np.prod([posprob[i] for i in xtest_idx])* 1e10 * pos * 1.0\n",
    "    denominator = np.prod([negprob[i] for i in xtest_idx])* 1e10 * neg * 1.0 \n",
    "    print(numerator)\n",
    "    print(denominator)\n",
    "    \n",
    "    return np.log(np.divide(numerator, denominator))\n",
    "\n",
    "#</GRADED>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "<p>(d) NaÃ¯ve Bayes can also be written as a linear classifier. Implement this in \n",
    "<b><code>naivebayesCL</code></b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#<GRADED>\n",
    "def naivebayesCL(x,y):\n",
    "    \"\"\"\n",
    "    function [w,b]=naivebayesCL(x,y);\n",
    "    Implementation of a Naive Bayes classifier\n",
    "    Input:\n",
    "    x : n input vectors of d dimensions (nxd)\n",
    "    y : n labels (-1 or +1)\n",
    "\n",
    "    Output:\n",
    "    w : weight vector of d dimensions\n",
    "    b : bias (scalar)\n",
    "    \"\"\"\n",
    "    \n",
    "    n, d = x.shape\n",
    "    ## fill in code here\n",
    "    posprob, negprob = naivebayesPXY(x,y)\n",
    "    pos, neg = naivebayesPY(x,y)\n",
    "    w = [np.log(np.divide(pos1, neg1)) for pos1, neg1 in zip(posprob, negprob)]\n",
    "    b = np.log(np.divide(pos, neg))\n",
    "    return w, b \n",
    "    \n",
    "\n",
    "#</GRADED>\n",
    "\n",
    "w,b = naivebayesCL(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "<p>(e) Implement \n",
    "<b><code>classifyLinear</code></b>\n",
    " that applies a linear weight vector and bias to a set of input vectors and outputs their predictions.  (You can use your answer from the previous project.)\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error: 24.17%\n"
     ]
    }
   ],
   "source": [
    "#<GRADED>\n",
    "from sklearn import tree\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "def classifyLinear(x,w,b=0):\n",
    "    \"\"\"\n",
    "    function preds=classifyLinear(x,w,b);\n",
    "    \n",
    "    Make predictions with a linear classifier\n",
    "    Input:\n",
    "    x : n input vectors of d dimensions (nxd)\n",
    "    w : weight vector of d dimensions\n",
    "    b : bias (optional)\n",
    "    \n",
    "    Output:\n",
    "    preds: predictions\n",
    "    \"\"\"\n",
    "    \n",
    "    ## fill in code here\n",
    "    result = np.matmul(x, w) + b\n",
    "    pred = []\n",
    "    for num in result:\n",
    "        if num > 0.15:\n",
    "            pred.append(1)\n",
    "        else:\n",
    "            pred.append(-1)\n",
    "    \n",
    "    return np.array(pred)\n",
    "        \n",
    "        \n",
    "\n",
    "#</GRADED>\n",
    "\n",
    "print('Training error: %.2f%%' % (100 *(classifyLinear(X, w, b) != Y).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "You can now test your code with the following interactive name classification script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data ...\n",
      "Training classifier ...\n",
      "Training error: 24.17%\n",
      "Please enter your name>\n",
      "Rachel\n",
      "Rachel, I am sure you are a nice boy.\n",
      "\n",
      "Please enter your name>\n",
      "Howell\n",
      "Howell, I am sure you are a nice boy.\n",
      "\n",
      "Please enter your name>\n",
      "Ian\n",
      "Ian, I am sure you are a nice boy.\n",
      "\n",
      "Please enter your name>\n",
      "Ling\n",
      "Ling, I am sure you are a nice boy.\n",
      "\n",
      "Please enter your name>\n",
      "john\n",
      "john, I am sure you are a nice girl.\n",
      "\n",
      "Please enter your name>\n"
     ]
    }
   ],
   "source": [
    "DIMS = 128\n",
    "print('Loading data ...')\n",
    "X,Y = genTrainFeatures(DIMS)\n",
    "print('Training classifier ...')\n",
    "w,b=naivebayesCL(X,Y)\n",
    "error = np.mean(classifyLinear(X,w,b) != Y)\n",
    "print('Training error: %.2f%%' % (100 * error))\n",
    "\n",
    "while True:\n",
    "    print('Please enter your name>')\n",
    "    yourname = input()\n",
    "    if len(yourname) < 1:\n",
    "        break\n",
    "    xtest = name2features(yourname,B=DIMS,LoadFile=False)\n",
    "    pred = classifyLinear(xtest,w,b)[0]\n",
    "    if pred > 0:\n",
    "        print(\"%s, I am sure you are a nice boy.\\n\" % yourname)\n",
    "    else:\n",
    "        print(\"%s, I am sure you are a nice girl.\\n\" % yourname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "<h3> Feature Extraction (Competition)</h3>\n",
    "\n",
    "<p>(e) (<b>optional</b>) As always, this programming project also includes a competition.  We will rank all submissions by how well your Na&iuml;ve Bayes classifier performs on a secret test set. If you want to improve your classifier modify <code>name2features2</code> below.   The automatic reader will use your Python script to extract features and train your classifier on the same names training set by calling the function with only one argument--the name of a file containing a list of names.  The given implementation is the same as the given <code>name2features</code> above.\n",
    "</p>\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#<GRADED>\n",
    "def hashfeatures(baby, B, FIX):\n",
    "    v = np.zeros(B)\n",
    "    for m in range(FIX):\n",
    "        featurestring = \"prefix\" + baby[:m]\n",
    "        v[hash(featurestring) % B] = 1\n",
    "        featurestring = \"suffix\" + baby[-1*m:]\n",
    "        v[hash(featurestring) % B] = 1\n",
    "    return v\n",
    "\n",
    "def name2features2(filename, B=124500, FIX=5, LoadFile=True):\n",
    "    \"\"\"\n",
    "    Output:\n",
    "    X : n feature vectors of dimension B, (nxB)\n",
    "    \"\"\"\n",
    "    print(B, FIX)\n",
    "    # read in baby names\n",
    "    if LoadFile:\n",
    "        with open(filename, 'r') as f:\n",
    "            babynames = [x.rstrip() for x in f.readlines() if len(x) > 0]\n",
    "    else:\n",
    "        babynames = filename.split('\\n')\n",
    "    n = len(babynames)\n",
    "    X = np.zeros((n, B))\n",
    "    for i in range(n):\n",
    "        X[i,:] = hashfeatures(babynames[i], B, FIX)\n",
    "    return X\n",
    "\n",
    "#</GRADED>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Credits</h4>\n",
    "  Parts of this webpage were copied from or heavily inspired by John DeNero's and Dan Klein's (awesome) <a href=\"http://ai.berkeley.edu/project_overview.html\">Pacman class</a>. The name classification idea originates from <a href=\"http://nickm.com\">Nick Montfort</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
